{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1533d2",
   "metadata": {},
   "source": [
    "# Dirty Dataset\n",
    "This example illustrate how to perform meta-blocking on a dirty dataset (data deduplication), so when we have one dataset that contains duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b84809",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "sparkER provides wrappers to load CSV and JSON files.\n",
    "\n",
    "*real_id_field* is the field that contains the identifier of the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiles contained in the first dataset\n",
    "profiles = sparker.CSVWrapper.load_profiles('./datasets/dirty/cora/cora.csv', real_id_field = \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_pdf = profiles.toDF().toPandas()\n",
    "profiles_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd62ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_pdf[\"attributes\"].to_dict()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b54c1",
   "metadata": {},
   "source": [
    "Let's visualize a profile to check if they are correctly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profiles.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba53b12",
   "metadata": {},
   "source": [
    "Extract the max id (it will be used later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef815292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max profile id\n",
    "max_profile_id = profiles.map(lambda profile: profile.profile_id).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf8051",
   "metadata": {},
   "source": [
    "### Groundtruth (optional)\n",
    "If you have a groundtruth you can measure the performance of each step.\n",
    "\n",
    "When you load the groundtruth it contains the original profiles IDs, it is necessary to convert it to use the IDs assigned to each profile by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the groundtruth, takes as input the path of the file and the names of the attributes that represent\n",
    "# respectively the id of profiles of the first dataset and the id of profiles of the second dataset\n",
    "gt = sparker.CSVWrapper.load_groundtruth('./datasets/dirty/cora/groundtruth.csv', 'id1', 'id2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the groundtruth by replacing original IDs with those given by Spark\n",
    "new_gt = sparker.Converters.convert_groundtruth(gt, profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can explore some pairs\n",
    "random.sample(sorted(new_gt), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232a623",
   "metadata": {},
   "source": [
    "## Blocking\n",
    "Now we can perform blocking.\n",
    "\n",
    "By default sparkER performs token blocking, but it is possible to provide a different blocking function.\n",
    "\n",
    "In the following example each token is splitted in ngrams of size 4 that are used for blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = sparker.Blocking.create_blocks(profiles,\n",
    "                                        blocking_method=sparker.BlockingKeysStrategies.ngrams_blocking,\n",
    "                                        ngram_size=4)\n",
    "print(\"Number of blocks\",blocks.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b4a70",
   "metadata": {},
   "source": [
    "Let's continue by using the standard token blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = sparker.Blocking.create_blocks(profiles)\n",
    "print(\"Number of blocks\",blocks.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca826a0b",
   "metadata": {},
   "source": [
    "## Block cleaning\n",
    "\n",
    "sparkER implements two block cleaning strategies:\n",
    "\n",
    "* Block purging: discard the largest blocks that involve too many comparisons, the parameter must be >= 1. A lower value mean a more aggressive purging.\n",
    "* Block cleaning: removes for every profile the largest blocks in which it appears. The parameter is in range ]0, 1\\[. A lower value mean a more aggressive cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be26d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfoms the purging\n",
    "blocks_purged = sparker.BlockPurging.block_purging(blocks, 1.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs the cleaning\n",
    "(profile_blocks, profile_blocks_filtered, blocks_after_filtering) = sparker.BlockFiltering.block_filtering_quick(blocks_purged, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981401c0",
   "metadata": {},
   "source": [
    "If you have the groundtruth, after every blocking step it is possible to check which are the performance of the blocking collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5c7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, precision, cmp_n = sparker.Utils.get_statistics(blocks_after_filtering, max_profile_id, new_gt)\n",
    "\n",
    "print(\"Recall\", recall)\n",
    "print(\"Precision\", precision)\n",
    "print(\"Number of comparisons\", cmp_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a531a8",
   "metadata": {},
   "source": [
    "## Meta-blocking\n",
    "Meta-blocking can be used to further refine the block collection removing superfluous comparisons.\n",
    "\n",
    "SparkER implements different kind of meta-blocking algorithms, you can find the descriptions in our paper.\n",
    "\n",
    "\n",
    "For every partition of the RDD the pruning algorithm returns as output a triplet that contains:\n",
    "\n",
    "* The number of edges\n",
    "* The number of matches (only if the groundtruth is provided)\n",
    "* The retained edges\n",
    "\n",
    "To perform the meta-blocking first some data structures have to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "block_index_map = blocks_after_filtering.map(lambda b : (b.block_id, b.profiles)).collectAsMap()\n",
    "block_index = sc.broadcast(block_index_map)\n",
    "\n",
    "# This is only needed for certain weight measures\n",
    "profile_blocks_size_index = sc.broadcast(profile_blocks_filtered.map(lambda pb : (pb.profile_id, len(pb.blocks))).collectAsMap())\n",
    "\n",
    "# Broadcasted groundtruth\n",
    "gt_broadcast = sc.broadcast(new_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647641b0",
   "metadata": {},
   "source": [
    "### Weighted Node Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf414b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparker.WNP.wnp(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          comparison_type=sparker.ComparisonTypes.OR\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec31755",
   "metadata": {},
   "source": [
    "### Reciprocal Weighted Node Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd75b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparker.WNP.wnp(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          comparison_type=sparker.ComparisonTypes.AND\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07c3e6",
   "metadata": {},
   "source": [
    "### Weighted Edge Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32079453",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparker.WEP.wep(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d580626",
   "metadata": {},
   "source": [
    "### Cardinality Node Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparker.CNP.cnp(\n",
    "                          blocks_after_filtering,\n",
    "                          profiles.count(),\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          comparison_type=sparker.ComparisonTypes.OR\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541f89c",
   "metadata": {},
   "source": [
    "### Reciprocal Cardinality Node Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparker.CNP.cnp(\n",
    "                          blocks_after_filtering,\n",
    "                          profiles.count(),\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          comparison_type=sparker.ComparisonTypes.AND\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5285b5",
   "metadata": {},
   "source": [
    "### Cardinality Edge Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparker.CEP.cep(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a09f1",
   "metadata": {},
   "source": [
    "## Collecting edges after meta-blocking\n",
    "As mentioned before, the third element of the tuples returned by the meta-blocking contains the edges.\n",
    "\n",
    "\n",
    "Edges are weighted according to the weight strategy provided to the meta-blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c96106d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 785, 8),\n",
       " (0, 787, 8),\n",
       " (0, 790, 8),\n",
       " (0, 794, 8),\n",
       " (0, 442, 8),\n",
       " (780, 781, 12),\n",
       " (780, 782, 13),\n",
       " (780, 783, 9),\n",
       " (780, 785, 10),\n",
       " (780, 786, 11)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = results.flatMap(lambda x: x[2])\n",
    "\n",
    "edges.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d86827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
